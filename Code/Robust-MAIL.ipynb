{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7957692,"sourceType":"datasetVersion","datasetId":4680816},{"sourceId":10653461,"sourceType":"datasetVersion","datasetId":6597049},{"sourceId":13343393,"sourceType":"datasetVersion","datasetId":8461487},{"sourceId":13359627,"sourceType":"datasetVersion","datasetId":8473831},{"sourceId":13359683,"sourceType":"datasetVersion","datasetId":8473861},{"sourceId":13360427,"sourceType":"datasetVersion","datasetId":8474376},{"sourceId":13365289,"sourceType":"datasetVersion","datasetId":8478218}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nsns.set_style('whitegrid')\nfrom sklearn.metrics import confusion_matrix , classification_report\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense , Flatten , Conv2D , MaxPooling2D , Dropout , Activation , BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam , Adamax\nfrom tensorflow.keras import regularizers\n\n#Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n#tf.keras.mixed_precision.set_global_policy('mixed_float16')\n\nimport tensorflow as tf\nimport numpy as np\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, ReLU, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import KLDivergence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications import DenseNet121, ResNet50V2\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nimport copy\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, ReLU, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import KLDivergence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications import DenseNet169, MobileNetV2, ResNet50, EfficientNetB0\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nimport copy\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modality 1","metadata":{}},{"cell_type":"code","source":"X_train_h = np.load('x_train.npy')\ny_train_h = np.load('y_train.npy')\nX_test_h = np.load('x_test(1).npy')\ny_test_h = np.load('y_test(1).npy')\n\nX_val_h = np.load('x_val.npy')\ny_val_h = np.load('y_val.npy')\n\n\nX_train_h.shape, y_train_h.shape, X_test_h.shape, y_test_h.shape, X_val_h.shape, y_val_h.shape\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_indices = np.random.choice(1001, 810, replace=False)\n\nX_test_h1 = X_test_h[random_indices]\ny_test_h1 = y_test_h[random_indices]\n\nX_test_h1.shape, y_test_h1.shape, X_test_h.shape, y_test_h.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_indices = np.random.choice(1002, 648, replace=False)\n\nX_val_h1 = X_val_h[random_indices]\ny_val_h1 = y_val_h[random_indices]\n\nX_test_h1.shape, y_test_h1.shape, X_test_h.shape, y_test_h.shape, X_val_h1.shape, y_val_h1.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_h.shape, y_train_h.shape, X_test_h.shape, y_test_h.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modality 2","metadata":{}},{"cell_type":"code","source":"X_train_s = np.load('X.npy')\ny_train_s = np.load('Y.npy')\n\nX_train_s.shape, y_train_s.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_train_s, y_train_s, test_size=0.2, random_state=42)\nX_train_s, X_val_s, y_train_s, y_val_s = train_test_split(X_train_s, y_train_s, test_size=0.2, random_state=42)\n\nX_train_s.shape,X_test_s.shape, y_train_s.shape,y_test_s.shape, y_val_s.shape,y_val_s.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\n\ndef rotate_image(image, angle):\n    \"\"\"\n    Rotate the image by the specified angle.\n    \"\"\"\n    center = tuple(np.array(image.shape[1::-1]) / 2)\n    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n    return rotated_image\n\ndef translate_image(image, tx, ty):\n    \"\"\"\n    Translate the image by the specified translation parameters.\n    \"\"\"\n    translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n    translated_image = cv2.warpAffine(image, translation_matrix, image.shape[1::-1])\n    return translated_image\n\ndef apply_gaussian_blur(image, kernel_size=3):\n    \"\"\"\n    Apply Gaussian Blur to the image to reduce noise and improve generalization.\n    \"\"\"\n    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n    return blurred_image\n\n# Augmentation parameters\nrotation_angles = [20]\ntranslations = [(5, 5)]\nkernel_sizes = [3]  # Gaussian Blur kernel sizes\n\naugmented_X_train = []\naugmented_y_train = []\n\nfor image, label in zip(X_train_s, y_train_s):\n    # Augment with rotations\n    for angle in rotation_angles:\n        rotated_image = rotate_image(image, angle)\n        augmented_X_train.append(rotated_image)\n        augmented_y_train.append(label)\n\n    # Augment with translations\n    for tx, ty in translations:\n        translated_image = translate_image(image, tx, ty)\n        augmented_X_train.append(translated_image)\n        augmented_y_train.append(label)\n\n    # Augment with Gaussian Blur\n    for kernel_size in kernel_sizes:\n        blurred_image = apply_gaussian_blur(image, kernel_size)\n        augmented_X_train.append(blurred_image)\n        augmented_y_train.append(label)\n\n# Convert lists to numpy arrays\naugmented_X_train = np.array(augmented_X_train)\naugmented_y_train = np.array(augmented_y_train)\n\n# Shuffle the data\nshuffle_indices = np.random.permutation(len(augmented_X_train))\naugmented_X_train = augmented_X_train[shuffle_indices]\naugmented_y_train = augmented_y_train[shuffle_indices]\naugmented_X_train.shape, augmented_y_train.shape\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_indices = np.random.choice(7773, 5421, replace=False)\n\naugmented_X_train = augmented_X_train[random_indices]\naugmented_y_train = augmented_y_train[random_indices]\n\naugmented_X_train.shape, augmented_y_train.shape\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_s = np.concatenate((X_train_s, augmented_X_train), axis=0)\ny_train_s = np.concatenate((y_train_s, augmented_y_train), axis=0)\nX_train_s.shape, y_train_s.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_s.shape, y_train_s.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n\ndef multi_kernel_groupwise_conv2(x, filters, groups=16, strides=1):\n    # 1x1 Group-wise Convolution\n    conv1x1 = layers.DepthwiseConv2D(kernel_size=1, strides=strides, padding=\"same\")(x)\n\n    # 3x3 Group-wise Convolution\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=5, strides=strides, padding=\"same\")(x)\n\n    # 5x5 Group-wise Convolution\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=7, strides=strides, padding=\"same\")(x)\n\n    # Depthwise 3x3 Group-wise Convolution\n    #depthwise3x3 = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n    \n    # Concatenate all outputs along the channel axis\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")(x1)\n\n    x = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(x)\n    x = layers.Add()([x, x1])\n    x = layers.Activation('relu')(x)\n    \n    return x\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nclass MultiKernelGroupwiseConv2(layers.Layer):\n    def __init__(self, filters, groups=16, strides=1, **kwargs):\n        super(MultiKernelGroupwiseConv2, self).__init__(**kwargs)\n        self.filters = filters\n        self.groups = groups\n        self.strides = strides\n\n        # 1x1 Group-wise Convolution\n        self.conv1x1 = layers.DepthwiseConv2D(kernel_size=1, strides=strides, padding=\"same\")\n\n        # 3x3 Group-wise Convolution\n        self.conv3x3 = layers.DepthwiseConv2D(kernel_size=5, strides=strides, padding=\"same\")\n\n        # 5x5 Group-wise Convolution\n        self.conv5x5 = layers.DepthwiseConv2D(kernel_size=7, strides=strides, padding=\"same\")\n\n        # Final 1x1 Group-wise Convolution\n        self.final_conv = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")\n\n        # Shortcut Path\n        self.shortcut_conv = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')\n\n        # Activation Function\n        self.activation = layers.Activation('relu')\n\n    def call(self, inputs):\n        conv1x1 = self.conv1x1(inputs)\n        conv3x3 = self.conv3x3(inputs)\n        conv5x5 = self.conv5x5(inputs)\n\n        # Concatenate along the channel axis\n        x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n        x1 = self.final_conv(x1)\n\n        # Shortcut connection\n        x = self.shortcut_conv(inputs)\n        x = layers.Add()([x, x1])\n        x = self.activation(x)\n\n        return x\n\n\n# Custom Attention Block with Global Pooling\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\n\n\n\n# Residual Attention Block (Using Spatial Attention)\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef multi_kernel_groupwise_conv1(x, filters, groups=8, strides=2):\n    \"\"\" Advanced Multi-Kernel Groupwise Convolution Block (Without Attention, Optimized) \"\"\"\n\n    # Multi-Kernel Parallel Convolutions with Different Receptive Fields\n    conv1x1 = layers.Conv2D(filters // 4, kernel_size=1, padding=\"same\")(x)\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x)\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=5, padding=\"same\")(x)\n    conv_dilated = layers.DepthwiseConv2D(kernel_size=3, dilation_rate=2, padding=\"same\")(x)\n\n    # Feature Fusion via Concatenation\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5, conv_dilated])\n\n    # Channel Shuffle for Better Feature Mixing\n    def channel_shuffle(x, groups):\n        batch, height, width, channels = tf.unstack(tf.shape(x))\n        x = tf.reshape(x, [-1, height, width, groups, channels // groups])\n        x = tf.transpose(x, [0, 1, 2, 4, 3])\n        x = tf.reshape(x, [-1, height, width, channels])\n        return x\n\n    x1 = layers.Lambda(lambda x: channel_shuffle(x, groups))(x1)\n\n    # Depthwise + Grouped Convolutions Hybrid\n    x1 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x1)\n    x1 = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x1)\n\n    # Downsampling x1\n    x1 = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x1)\n\n    # **Fix: Ensure x has the same number of channels as x1**\n    x = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n    x = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n\n    # Residual Connection\n    x = layers.Add()([x, x1])\n    x = layers.Activation(\"relu\")(x)\n    \n    return x\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n## MSGCA\n'''def multi_kernel_groupwise_conv(x, filters, groups=8, strides=1, use_se=True):\n    # GPC for MSGDC\n    conv1x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, strides=strides, padding=\"same\", use_bias=False)(x)\n    \n\n    # DDC (Multi-scale receptive fields) for MSGDC\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=3, dilation_rate=1, strides=strides, padding=\"same\", use_bias=False)(x)\n    \n\n    # DWC (Multi-scale receptive fields) for MSGDC\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=5, strides=strides, padding=\"same\", use_bias=False)(x)\n    \n\n    # Concatenation and 1x1 Fusion for MSHC\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n\n    def channel_shuffle(x, groups):\n        batch, height, width, channels = tf.unstack(tf.shape(x))\n        x = tf.reshape(x, [-1, height, width, groups, channels // groups])\n        x = tf.transpose(x, [0, 1, 2, 4, 3])\n        x = tf.reshape(x, [-1, height, width, channels])\n        return x\n\n    x1 = layers.Lambda(lambda x: channel_shuffle(x, groups))(x1)\n    \n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\", use_bias=False)(x1)\n    x1 = layers.BatchNormalization()(x1)\n\n    # CA layer\n\n    GMN = tf.reduce_min(inputs, axis=[1, 2])(x1)\n    GMP = tf.reduce_max(inputs, axis=[1, 2])(x1)\n    GAP = tf.reduce_mean(inputs, axis=[1, 2])(x1)\n    GSP = tf.reduce_suminputs, axis=[1, 2])(x1)\n\n    \n    channel_add = GMN + GAP + GMP + GSP\n\n    channel_sub = GMP - GAP - GMN\n    \n    channel_info = channel_add + channel_sub\n\n    se = layers.Dense(filters // 16, activation='relu', use_bias=False)(div_channel)\n    se = layers.Dense(filters, activation='sigmoid', use_bias=False)(se)\n\n    se = layers.Reshape((1, 1, filters))(se)\n    \n    self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='global_scale')\n    se *= self.global_scale\n    se = layers.Activation('sigmoid')(se)\n    \n    \n    x1 = layers.Multiply()([x1, se])\n\n    \n    # Residual Connection\n    x = layers.Conv2D(filters=filters, kernel_size=1, strides=strides, padding='same', use_bias=False)(x)\n    \n\n    x = layers.Add()([x, x1])\n    x = layers.Activation('relu')(x)  # GELU activation for better convergence\n\n    return x\n'''\n\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# ---------------------------\n# Utility\n# ---------------------------\ndef _assert_divisible(a, b, name):\n    if a % b != 0:\n        raise ValueError(f\"{name}: {a} must be divisible by {b}\")\n\n# ---------------------------\n# Random Projection Conv2D  (GPC with RPF)\n# ---------------------------\nclass RandomProjectionConv2D(layers.Layer):\n    \"\"\"\n    Grouped Conv2D where a subset of output channels per group is replaced\n    by re-sampled Gaussian random projection filters at every forward pass.\n\n    Args:\n        filters: total output channels.\n        kernel_size: int or (h, w).\n        strides: int or (sh, sw).\n        padding: \"same\" or \"valid\".\n        groups: number of groups for grouped convolution (>=1).\n        use_bias: bool.\n        rpf_ratio: fraction in [0,1] of output channels PER GROUP replaced by RPF.\n        rpf_l2: L2 coefficient applied ONLY on trainable subset (Eq. 8 in CVPR'23).\n        rpf_stddev: stddev for Gaussian weights. If None, uses 1/sqrt(r^2) (≈1/r).\n    \"\"\"\n    def __init__(self,\n                 filters,\n                 kernel_size,\n                 strides=1,\n                 padding=\"same\",\n                 groups=1,\n                 use_bias=True,\n                 rpf_ratio=0.0,\n                 rpf_l2=0.0,\n                 rpf_stddev=None,\n                 name=None,\n                 **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.filters = int(filters)\n        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else tuple(kernel_size)\n        self.strides = (strides, strides) if isinstance(strides, int) else tuple(strides)\n        self.padding = padding.upper()\n        self.groups = int(groups)\n        self.use_bias = use_bias\n\n        self.rpf_ratio = float(rpf_ratio)\n        self.rpf_l2 = float(rpf_l2)\n        self.rpf_stddev = rpf_stddev  # if None -> computed in build\n\n        # Base trainable conv across all filters (we will mask regularization later)\n        self.base_conv = layers.Conv2D(\n            filters=self.filters,\n            kernel_size=self.kernel_size,\n            strides=self.strides,\n            padding=padding,\n            groups=self.groups,\n            use_bias=self.use_bias\n        )\n\n    def build(self, input_shape):\n        in_channels = int(input_shape[-1])\n        _assert_divisible(in_channels, self.groups, \"Input channels for grouped Conv2D\")\n        _assert_divisible(self.filters, self.groups, \"Filters for grouped Conv2D\")\n\n        self.base_conv.build(input_shape)  # create weights\n\n        kh, kw = self.kernel_size\n        self._std = self.rpf_stddev if self.rpf_stddev is not None else (1.0 / float(kh * kw)) ** 0.5\n\n        # Precompute per-group counts\n        self._cin_per_group = in_channels // self.groups\n        self._fpg = self.filters // self.groups  # filters per group\n        self._rpg = int(round(self.rpf_ratio * self._fpg))  # random per group\n        self._tpg = self._fpg - self._rpg                       # trainable per group\n\n        # Build a binary mask (1=trainable, 0=random) aligned with base_conv.kernel\n        # base_conv.kernel shape: [kh, kw, Cin/groups, filters]\n        # We concat per group along last axis: [ ... , fpg ] x groups\n        if self._rpg > 0:\n            train_block = tf.ones((kh, kw, self._cin_per_group, self._tpg), dtype=self.base_conv.kernel.dtype)\n            rand_block  = tf.zeros((kh, kw, self._cin_per_group, self._rpg), dtype=self.base_conv.kernel.dtype)\n            per_group_mask = tf.concat([train_block, rand_block], axis=-1)\n        else:\n            per_group_mask = tf.ones((kh, kw, self._cin_per_group, self._fpg), dtype=self.base_conv.kernel.dtype)\n\n        self._train_mask = tf.concat([per_group_mask] * self.groups, axis=-1)  # [kh,kw,Cin/groups,filters]\n\n        # Optional: L2 on the trainable part only\n        if self.rpf_l2 > 0.0:\n            self.add_loss(self.rpf_l2 * tf.nn.l2_loss(self.base_conv.kernel * self._train_mask))\n\n        super().build(input_shape)\n\n    def call(self, x):\n        # Fast path: ratio=0 means \"just a grouped conv\"\n        if self._rpg <= 0:\n            return self.base_conv(x)\n\n        # First, run the base grouped conv (already adds bias if any)\n        y_train_all = self.base_conv(x)  # [B,H,W,filters]\n\n        # Prepare group splits on input and train output\n        x_groups = tf.split(x, self.groups, axis=-1)\n        y_groups = tf.split(y_train_all, self.groups, axis=-1)\n\n        kh, kw = self.kernel_size\n        stride = [1, self.strides[0], self.strides[1], 1]\n\n        out_groups = []\n        for g, (xg, yg) in enumerate(zip(x_groups, y_groups)):\n            # Keep first 'tpg' channels from trainable output\n            y_tr = yg[:, :, :, :self._tpg]\n\n            # Compute random-projection outputs for the last 'rpg' channels\n            if self._rpg > 0:\n                kr = tf.random.normal(\n                    shape=(kh, kw, self._cin_per_group, self._rpg),\n                    mean=0.0, stddev=self._std, dtype=x.dtype\n                )\n                yr = tf.nn.conv2d(\n                    xg, kr, strides=stride, padding=self.padding, data_format=\"NHWC\", dilations=[1, 1, 1, 1]\n                )\n            else:\n                yr = tf.zeros_like(y_tr)[:, :, :, :0]  # empty\n\n            out_groups.append(tf.concat([y_tr, yr], axis=-1))\n\n        return tf.concat(out_groups, axis=-1)\n\n    def get_config(self):\n        cfg = super().get_config()\n        cfg.update(dict(\n            filters=self.filters,\n            kernel_size=self.kernel_size,\n            strides=self.strides,\n            padding=self.padding,\n            groups=self.groups,\n            use_bias=self.use_bias,\n            rpf_ratio=self.rpf_ratio,\n            rpf_l2=self.rpf_l2,\n            rpf_stddev=self.rpf_stddev,\n        ))\n        return cfg\n\n# ---------------------------\n# Random Projection DepthwiseConv2D  (DWC with RPF)\n# ---------------------------\nclass RandomProjectionDepthwiseConv2D(layers.Layer):\n    \"\"\"\n    Depthwise Conv2D where a subset of input channels is replaced by\n    re-sampled Gaussian random depthwise kernels each forward pass.\n\n    Args:\n        kernel_size, strides, padding, depth_multiplier, use_bias: standard.\n        rpf_ratio: fraction in [0,1] of input channels to randomize.\n        rpf_l2: L2 coefficient only on trainable subset.\n        rpf_stddev: stddev for Gaussian weights. If None, uses 1/sqrt(r^2).\n    \"\"\"\n    def __init__(self,\n                 kernel_size,\n                 strides=1,\n                 padding=\"same\",\n                 depth_multiplier=1,\n                 use_bias=True,\n                 rpf_ratio=0.0,\n                 rpf_l2=0.0,\n                 rpf_stddev=None,\n                 name=None,\n                 **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else tuple(kernel_size)\n        self.strides = (strides, strides) if isinstance(strides, int) else tuple(strides)\n        self.padding = padding.upper()\n        self.depth_multiplier = int(depth_multiplier)\n        self.use_bias = use_bias\n\n        self.rpf_ratio = float(rpf_ratio)\n        self.rpf_l2 = float(rpf_l2)\n        self.rpf_stddev = rpf_stddev\n\n        self.base_dw = layers.DepthwiseConv2D(\n            kernel_size=self.kernel_size,\n            strides=self.strides,\n            padding=padding,\n            depth_multiplier=self.depth_multiplier,\n            use_bias=self.use_bias\n        )\n\n    def build(self, input_shape):\n        in_channels = int(input_shape[-1])\n        self.base_dw.build(input_shape)\n\n        kh, kw = self.kernel_size\n        self._std = self.rpf_stddev if self.rpf_stddev is not None else (1.0 / float(kh * kw)) ** 0.5\n\n        # how many input channels to randomize (last ones by convention)\n        self._n_rand = int(round(self.rpf_ratio * in_channels))\n        self._n_train = in_channels - self._n_rand\n\n        if self._n_rand > 0:\n            # channel mask: shape [Cin], 0=trainable, 1=random\n            zeros = tf.zeros([self._n_train], dtype=self.base_dw.depthwise_kernel.dtype)\n            ones  = tf.ones([self._n_rand], dtype=self.base_dw.depthwise_kernel.dtype)\n            self._rand_mask_vec = tf.concat([zeros, ones], axis=0)               # [Cin]\n            self._rand_mask = tf.reshape(self._rand_mask_vec, [1, 1, in_channels, 1])  # broadcast to kernel/output\n        else:\n            self._rand_mask_vec = tf.zeros([in_channels], dtype=self.base_dw.depthwise_kernel.dtype)\n            self._rand_mask = tf.zeros([1, 1, in_channels, 1], dtype=self.base_dw.depthwise_kernel.dtype)\n\n        # Optional: L2 only on trainable subset of the depthwise kernel\n        if self.rpf_l2 > 0.0 and self._n_train > 0:\n            train_mask = 1.0 - self._rand_mask\n            self.add_loss(self.rpf_l2 * tf.nn.l2_loss(self.base_dw.depthwise_kernel * train_mask))\n\n        super().build(input_shape)\n\n    def call(self, x):\n        # Fast path: ratio=0 means \"just a depthwise conv\"\n        if self._n_rand <= 0:\n            return self.base_dw(x)\n\n        kh, kw = self.kernel_size\n        dm = self.depth_multiplier\n        # Trainable part (for channels where mask==0) comes from base_dw.depthwise_kernel\n        train_kernel = self.base_dw.depthwise_kernel\n        train_mask = 1.0 - self._rand_mask\n\n        # Random kernel for the masked channels; zeros elsewhere\n        rand_k = tf.random.normal(\n            shape=(kh, kw, int(x.shape[-1]), dm), mean=0.0, stddev=self._std, dtype=x.dtype\n        ) * self._rand_mask\n\n        # Compose mixed kernel\n        mixed_kernel = train_kernel * train_mask + rand_k\n\n        # Depthwise conv using assembled kernel\n        y = tf.nn.depthwise_conv2d(\n            x, mixed_kernel,\n            strides=[1, self.strides[0], self.strides[1], 1],\n            padding=self.padding,\n            data_format=\"NHWC\",\n            dilations=[1, 1]\n        )\n        if self.use_bias and self.base_dw.bias is not None:\n            y = tf.nn.bias_add(y, self.base_dw.bias, data_format=\"NHWC\")\n        return y\n\n    def get_config(self):\n        cfg = super().get_config()\n        cfg.update(dict(\n            kernel_size=self.kernel_size,\n            strides=self.strides,\n            padding=self.padding,\n            depth_multiplier=self.depth_multiplier,\n            use_bias=self.use_bias,\n            rpf_ratio=self.rpf_ratio,\n            rpf_l2=self.rpf_l2,\n            rpf_stddev=self.rpf_stddev,\n        ))\n        return cfg\n\n# ---------------------------\n# Channel shuffle\n# ---------------------------\ndef channel_shuffle(tensor, groups: int):\n    \"\"\"Shuffle channels (ShuffleNet-style).\"\"\"\n    b, h, w, c = tf.unstack(tf.shape(tensor))\n    tensor = tf.reshape(tensor, [b, h, w, groups, c // groups])\n    tensor = tf.transpose(tensor, [0, 1, 2, 4, 3])\n    tensor = tf.reshape(tensor, [b, h, w, c])\n    return tensor\n\n# ---------------------------\n# EMILA: Multi-kernel / group-wise conv with RPF in MSGDC\n# ---------------------------\nclass MultiKernelGroupWiseConv(layers.Layer):\n    \"\"\"\n    Multi-kernel / group-wise conv block with channel-shuffle,\n    global min/max/avg/sum attention, and residual connection.\n\n    RPF is injected into MSGDC sub-blocks:\n      - GPC 1x1 (grouped pointwise conv)\n      - DWC 3x3 and 5x5 (depthwise conv)\n    per Robust-MAIL design (Fig. 5E).  :contentReference[oaicite:4]{index=4}\n    \"\"\"\n    def __init__(self,\n                 filters: int,\n                 groups: int = 2,\n                 strides: int = 1,\n                 use_se: bool = True,\n                 reduction: int = 4,\n                 # --- RPF knobs (set ratio>0 to enable) ---\n                 rpf_ratio_gpc: float = 0.25,\n                 rpf_ratio_dwc: float = 0.25,\n                 rpf_l2: float = 1e-4,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.filters = filters\n        self.groups = groups\n        self.strides = strides\n        self.use_se = use_se\n        self.reduction = reduction\n        self.rpf_ratio_gpc = float(rpf_ratio_gpc)\n        self.rpf_ratio_dwc = float(rpf_ratio_dwc)\n        self.rpf_l2 = float(rpf_l2)\n\n        # ── convolutions (MSGDC) ───────────────────────────────────────────────\n        # 1×1 Grouped Pointwise with RPF (GPC)\n        self.conv1x1 = RandomProjectionConv2D(\n            filters, 1,\n            strides=strides,\n            padding=\"same\",\n            groups=groups,\n            use_bias=True,\n            rpf_ratio=self.rpf_ratio_gpc,\n            rpf_l2=self.rpf_l2,\n            name=\"rpf_gpc_1x1\"\n        )\n\n        # Depthwise 3×3 with RPF\n        self.dw3 = RandomProjectionDepthwiseConv2D(\n            3, strides=strides, padding=\"same\", use_bias=True,\n            rpf_ratio=self.rpf_ratio_dwc,\n            rpf_l2=self.rpf_l2,\n            name=\"rpf_dwc_3x3\"\n        )\n\n        # Depthwise 5×5 with RPF\n        self.dw5 = RandomProjectionDepthwiseConv2D(\n            5, strides=strides, padding=\"same\", use_bias=True,\n            rpf_ratio=self.rpf_ratio_dwc,\n            rpf_l2=self.rpf_l2,\n            name=\"rpf_dwc_5x5\"\n        )\n\n        # fusion after concat (leave standard conv unless you want RPF here too)\n        _assert_divisible(filters, 2, \"filters for fuse (groups=2)\")\n        self.fuse = layers.Conv2D(filters, 1, padding=\"same\", groups=2, use_bias=True)\n        self.bn_fuse = layers.BatchNormalization()\n\n        # Channel attention (CA)\n        self.fc1 = layers.Dense(filters // reduction, activation=\"relu\")\n        self.fc2 = layers.Dense(filters)\n        self.reshape = layers.Reshape((1, 1, filters))\n\n        # residual projection\n        self.proj = layers.Conv2D(filters, 1, strides=strides, padding=\"same\", use_bias=True)\n        self.bn_proj = layers.BatchNormalization()\n\n        self.act = layers.Activation(\"relu\")\n\n    # global trainable scale (as in your original)\n    def build(self, input_shape):\n        self.global_scale = self.add_weight(\n            name=\"global_scale\",\n            shape=(1, 1, 1, 1),\n            initializer=tf.keras.initializers.HeNormal(),\n            trainable=True,\n        )\n        super().build(input_shape)\n\n    def call(self, x):\n        # ── branch 1: grouped 1×1 (RPF-enabled) ──────────────────────────────\n        b1 = self.conv1x1(x)\n\n        # ── branch 2: dw-3×3 (RPF-enabled) ───────────────────────────────────\n        b2 = self.dw3(x)\n\n        # ── branch 3: dw-5×5 (RPF-enabled) ───────────────────────────────────\n        b3 = self.dw5(x)\n\n        # ── concat + shuffle + fuse ───────────────────────────────────────────\n        out = tf.concat([b1, b2, b3], axis=-1)\n        out = channel_shuffle(out, self.groups)\n        out = self.bn_fuse(self.fuse(out))\n\n        # ── CA  ───────────────────────────────────────────────────────────────\n        g_min = tf.reduce_min(out, axis=[1, 2])\n        g_max = tf.reduce_max(out, axis=[1, 2])\n        g_avg = tf.reduce_mean(out, axis=[1, 2])\n        g_sum = tf.reduce_sum(out, axis=[1, 2])\n\n        channel_add = g_min + g_avg + g_max + g_sum\n        channel_sub = g_max - g_avg - g_min\n        channel_info = channel_add + channel_sub\n\n        se = self.fc1(channel_info)\n        se = self.fc2(se)\n        se = self.reshape(se)               # (1,1,C)\n        se *= self.global_scale\n        se = tf.nn.sigmoid(se)\n\n        out *= se\n\n        # ── residual connection ───────────────────────────────────────────────\n        shortcut = self.bn_proj(self.proj(x))\n        out = self.act(shortcut + out)\n        return out\n\n# ---------------------------\n# ERLA / RGSA block (using the RPF-enabled EMILA)\n# ---------------------------\ndef RGSA(x,\n         filters,\n         strides=(1, 1),\n         use_projection=False,\n         # RPF knobs (pass ratio=0.0 to disable)\n         rpf_ratio_gpc=0.25,\n         rpf_ratio_dwc=0.25,\n         rpf_l2=1e-4):\n    \"\"\"\n    ERLA block with RPF-enabled EMILA inside (two stacked).\n    \"\"\"\n    shortcut = x\n\n    # First EMILA (MSGDC + CA), with optional downsampling\n    x = MultiKernelGroupWiseConv(filters=filters, groups=16, strides=strides,\n                                 rpf_ratio_gpc=rpf_ratio_gpc,\n                                 rpf_ratio_dwc=rpf_ratio_dwc,\n                                 rpf_l2=rpf_l2)(x)\n\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second EMILA (stride 1)\n    x = MultiKernelGroupWiseConv(filters=filters, groups=16, strides=(1, 1),\n                                 rpf_ratio_gpc=rpf_ratio_gpc,\n                                 rpf_ratio_dwc=rpf_ratio_dwc,\n                                 rpf_l2=rpf_l2)(x)\n\n    x = layers.Conv2D(filters, kernel_size=(1, 1), padding=\"same\")(x)  # PW conv for channel mixing\n    x = layers.BatchNormalization()(x)\n\n    # Adjust shortcut if needed\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding=\"same\")(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    # Residual\n    x = layers.Add()([x, shortcut])\n    x = layers.Activation('relu')(x)\n    return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Robust-MAIL ##","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, regularizers\n\n# ────────────────────────────\n# Utility: channel shuffle\n# ────────────────────────────\ndef channel_shuffle(tensor, groups: int):\n    \"\"\"Shuffle channels (ShuffleNet-style).\"\"\"\n    b, h, w, c = tf.unstack(tf.shape(tensor))\n    tensor = tf.reshape(tensor, [b, h, w, groups, c // groups])\n    tensor = tf.transpose(tensor, [0, 1, 2, 4, 3])\n    tensor = tf.reshape(tensor, [b, h, w, c])\n    return tensor\n\n\n# ────────────────────────────\n# RPF layer for Conv2D (1x1 / grouped)\n#   - Concatenates outputs of trainable Conv2D and a \"random\" Conv2D\n#   - Random Conv2D gets fresh Gaussian weights each call\n#   - Follows RPF: F_random ~ N(0, 1/r^2), r = kernel size (e.g., 1, 3, 5)\n# ────────────────────────────\n\nclass RPFConv2D(layers.Layer):\n    def __init__(self, filters, kernel_size,\n                 strides=1, padding=\"same\", groups=1,\n                 use_bias=True,\n                 rpf_ratio=0.5,\n                 train_l2=0.0,\n                 name=None):\n        super().__init__(name=name)\n        self.filters = int(filters)\n        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else tuple(kernel_size)\n        self.strides = (strides, strides) if isinstance(strides, int) else tuple(strides)\n        self.padding = padding\n        self.groups = int(groups)\n        self.use_bias = use_bias\n\n        # split filters into trainable vs random projections\n        self.n_rand = int(round(self.filters * float(rpf_ratio)))\n        self.n_train = self.filters - self.n_rand\n        self.train_l2 = float(train_l2)\n\n        self._conv_train = None\n        self._conv_rand  = None\n\n    def build(self, input_shape):\n        in_ch = int(input_shape[-1])\n\n        if self.n_train > 0:\n            self._conv_train = layers.Conv2D(\n                self.n_train, self.kernel_size,\n                strides=self.strides, padding=self.padding,\n                use_bias=self.use_bias, groups=self.groups,\n                kernel_regularizer=regularizers.l2(self.train_l2) if self.train_l2 > 0 else None,\n                name=f\"{self.name}_train\" if self.name else None\n            )\n            self._conv_train.build(input_shape)\n\n        if self.n_rand > 0:\n            # non-trainable conv; we will overwrite its kernel at each call with random Gaussian\n            self._conv_rand = layers.Conv2D(\n                self.n_rand, self.kernel_size,\n                strides=self.strides, padding=self.padding,\n                use_bias=self.use_bias, groups=self.groups,\n                trainable=False,\n                name=f\"{self.name}_rand\" if self.name else None\n            )\n            self._conv_rand.build(input_shape)\n\n        super().build(input_shape)\n\n    def _resample_random_kernel(self):\n        # stddev per RPF: N(0, 1/r^2); r is (square) kernel size (e.g., 1, 3, 5)\n        if self._conv_rand is None:\n            return\n        k_h = int(self.kernel_size[0])\n        stddev = 1.0 / float(k_h)  # variance 1/r^2\n        # Assign new random kernel each call\n        self._conv_rand.kernel.assign(\n            tf.random.normal(shape=tf.shape(self._conv_rand.kernel),\n                             mean=0.0, stddev=stddev, dtype=self._conv_rand.kernel.dtype)\n        )\n        if self.use_bias and self._conv_rand.bias is not None:\n            self._conv_rand.bias.assign(tf.zeros_like(self._conv_rand.bias))\n\n    def call(self, x, training=None):\n        y_parts = []\n        if self._conv_train is not None:\n            y_parts.append(self._conv_train(x, training=training))\n        if self._conv_rand is not None:\n            # ensure kernel exists, then resample before this forward\n            self._resample_random_kernel()\n            y_parts.append(self._conv_rand(x, training=training))\n        return y_parts[0] if len(y_parts) == 1 else tf.concat(y_parts, axis=-1)\n\n    def compute_output_shape(self, input_shape):\n        # Keras will infer H/W; we return channels explicitly\n        return tf.TensorShape([input_shape[0], input_shape[1], input_shape[2], self.filters])\n\n\n# ────────────────────────────\n# RPF layer for DepthwiseConv2D\n#   - Splits channels: first Cin_train go through trainable DWC,\n#     last Cin_rand go through a \"random\" DWC with resampled Gaussian kernel each call\n#   - Concatenate outputs along channels (keeps total channels Cin * depth_multiplier)\n# ────────────────────────────\nclass RPFDepthwiseConv2D(layers.Layer):\n    def __init__(self, kernel_size,\n                 strides=1, padding=\"same\",\n                 depth_multiplier=1,\n                 use_bias=True,\n                 rpf_ratio=0.5,      # fraction of input channels routed to random DW filters\n                 train_l2=0.0,\n                 name=None):\n        super().__init__(name=name)\n        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else tuple(kernel_size)\n        self.strides = (strides, strides) if isinstance(strides, int) else tuple(strides)\n        self.padding = padding\n        self.depth_multiplier = int(depth_multiplier)\n        self.use_bias = use_bias\n\n        self.rpf_ratio = float(rpf_ratio)\n        self.train_l2 = float(train_l2)\n\n        self._dw_train = None\n        self._dw_rand  = None\n\n        self._cin = None\n        self._cin_rand = None\n        self._cin_train = None\n\n    def build(self, input_shape):\n        self._cin = int(input_shape[-1])\n        self._cin_rand  = int(round(self._cin * self.rpf_ratio))\n        self._cin_train = self._cin - self._cin_rand\n\n        if self._cin_train > 0:\n            self._dw_train = layers.DepthwiseConv2D(\n                self.kernel_size, strides=self.strides, padding=self.padding,\n                depth_multiplier=self.depth_multiplier, use_bias=self.use_bias,\n                depthwise_regularizer=regularizers.l2(self.train_l2) if self.train_l2 > 0 else None,\n                name=f\"{self.name}_train\" if self.name else None\n            )\n            # build with a compatible (dummy) shape: [None, None, None, Cin_train]\n            self._dw_train.build(tf.TensorShape([None, None, None, self._cin_train]))\n\n        if self._cin_rand > 0:\n            self._dw_rand = layers.DepthwiseConv2D(\n                self.kernel_size, strides=self.strides, padding=self.padding,\n                depth_multiplier=self.depth_multiplier, use_bias=self.use_bias,\n                trainable=False,\n                name=f\"{self.name}_rand\" if self.name else None\n            )\n            self._dw_rand.build(tf.TensorShape([None, None, None, self._cin_rand]))\n\n        super().build(input_shape)\n\n    def _resample_random_kernel(self):\n        if self._dw_rand is None:\n            return\n        k_h = int(self.kernel_size[0])\n        stddev = 1.0 / float(k_h)  # variance 1/r^2\n        self._dw_rand.depthwise_kernel.assign(\n            tf.random.normal(shape=tf.shape(self._dw_rand.depthwise_kernel),\n                             mean=0.0, stddev=stddev, dtype=self._dw_rand.depthwise_kernel.dtype)\n        )\n        if self.use_bias and self._dw_rand.bias is not None:\n            self._dw_rand.bias.assign(tf.zeros_like(self._dw_rand.bias))\n\n    def call(self, x, training=None):\n        # Split channels: [trainable | random]\n        if self._cin_rand > 0 and self._cin_train > 0:\n            x_train, x_rand = tf.split(x, [self._cin_train, self._cin_rand], axis=-1)\n        elif self._cin_rand > 0:  # degenerate: all random\n            x_train, x_rand = None, x\n        else:                      # degenerate: all trainable\n            x_train, x_rand = x, None\n\n        y_parts = []\n        if x_train is not None and self._dw_train is not None:\n            y_parts.append(self._dw_train(x_train, training=training))\n        if x_rand is not None and self._dw_rand is not None:\n            self._resample_random_kernel()\n            y_parts.append(self._dw_rand(x_rand, training=training))\n        return y_parts[0] if len(y_parts) == 1 else tf.concat(y_parts, axis=-1)\n\n    def compute_output_shape(self, input_shape):\n        # depthwise: Cin * depth_multiplier channels\n        cin = int(input_shape[-1])\n        return tf.TensorShape([input_shape[0], input_shape[1], input_shape[2], cin * self.depth_multiplier])\n\n\n# ────────────────────────────\n# EMILA: Multi-kernel / group-wise conv with RPF on GPC (1x1) and DWC (3x3, 5x5)\n# ────────────────────────────\nclass MultiKernelGroupWiseConv(layers.Layer):\n    \"\"\"\n    Multi-kernel / group-wise conv block with channel-shuffle,\n    global min/max/avg/sum attention, and residual connection.\n    RPF integration:\n      - 1x1 grouped pointwise conv: fraction of output channels are random (rpf_ratio_gpc)\n      - 3x3 & 5x5 depthwise convs: fraction of input channels use random filters (rpf_ratio_dwc)\n    \"\"\"\n\n    def __init__(self,\n                 filters: int,\n                 groups: int = 2,\n                 strides: int = 1,\n                 use_se: bool = True,\n                 reduction: int = 4,\n                 # RPF knobs\n                 rpf_ratio_gpc: float = 0.0,   # 0..1; e.g. 0.25 means 25% random 1x1 filters\n                 rpf_ratio_dwc: float = 0.0,   # 0..1; e.g. 0.25 means 25% input channels random in DW convs\n                 rpf_l2: float = 0.0,          # extra L2 on trainable subset (Eq. 8 in RPF)\n                 spatial_noise_factor=0.6, channel_noise_factor=0.6,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.filters = int(filters)\n        self.groups = int(groups)\n        self.strides = (strides, strides) if isinstance(strides, int) else tuple(strides)\n        self.use_se = use_se\n        self.reduction = int(reduction)\n        self.rpf_ratio_gpc = float(rpf_ratio_gpc)\n        self.rpf_ratio_dwc = float(rpf_ratio_dwc)\n        self.rpf_l2 = float(rpf_l2)\n        self.spatial_noise_factor = spatial_noise_factor\n        self.channel_noise_factor = channel_noise_factor\n\n        # ── convolutions ─────────────────────────────────────\n        # 1×1 grouped (GPC) with optional RPF on output filters\n        self.conv1x1 = RPFConv2D(self.filters, 1,\n                                 strides=self.strides,\n                                 padding=\"same\",\n                                 groups=self.groups,\n                                 use_bias=True,\n                                 rpf_ratio=self.rpf_ratio_gpc,\n                                 train_l2=self.rpf_l2,\n                                 name=\"gpc1x1_rpf\")\n        self.bn1x1 = layers.BatchNormalization()\n\n        # Depthwise 3×3 with RPF over a fraction of input channels\n        self.dw3 = RPFDepthwiseConv2D(3, strides=self.strides,\n                                      padding=\"same\",\n                                      depth_multiplier=1,\n                                      use_bias=True,\n                                      rpf_ratio=self.rpf_ratio_dwc,\n                                      train_l2=self.rpf_l2,\n                                      name=\"dw3_rpf\")\n        self.bn_dw3 = layers.BatchNormalization()\n\n        # Depthwise 5×5 with RPF over a fraction of input channels\n        self.dw5 = RPFDepthwiseConv2D(5, strides=self.strides,\n                                      padding=\"same\",\n                                      depth_multiplier=1,\n                                      use_bias=True,\n                                      rpf_ratio=self.rpf_ratio_dwc,\n                                      train_l2=self.rpf_l2,\n                                      name=\"dw5_rpf\")\n        self.bn_dw5 = layers.BatchNormalization()\n\n        # Fusion 1×1 after concat (can remain light; no RPF needed here)\n        self.fuse = layers.Conv2D(self.filters, 1,\n                                  padding=\"same\",\n                                  groups=2,\n                                  name=\"fuse_1x1\")\n        self.bn_fuse = layers.BatchNormalization()\n\n        # Channel Attention (CA) MLP\n        self.fc1 = layers.Dense(self.filters // self.reduction, activation=\"relu\")\n        self.fc2 = layers.Dense(self.filters)\n        self.reshape = layers.Reshape((1, 1, self.filters))\n\n        # Residual projection if needed\n        self.proj = layers.Conv2D(self.filters, 1,\n                                  strides=self.strides,\n                                  padding=\"same\",\n                                  name=\"res_proj_1x1\")\n        self.bn_proj = layers.BatchNormalization()\n\n        self.act = layers.Activation(\"relu\")\n\n    def build(self, input_shape):\n        # global trainable scale used in your CA (requested in your earlier code)\n        self.global_scale = self.add_weight(\n            name=\"global_scale\",\n            shape=(1, 1, 1, 1),\n            initializer=tf.keras.initializers.HeNormal(),\n            trainable=True,\n        )\n        \n        self.spatial_noise_weight = self.add_weight(shape=(1,), initializer='ones', trainable=False)\n        self.channel_noise_weight = self.add_weight(shape=(1,), initializer='ones', trainable=False)\n        \n        super().build(input_shape)\n\n    def call(self, x, training=None):\n        # ── branch 1: grouped 1×1 (RPF-enabled) ─────────────\n        b1 = self.bn1x1(self.conv1x1(x, training=training), training=training)\n\n        # ── branch 2: depthwise 3×3 (RPF-enabled) ──────────\n        b2 = self.bn_dw3(self.dw3(x, training=training), training=training)\n\n        # ── branch 3: depthwise 5×5 (RPF-enabled) ──────────\n        b3 = self.bn_dw5(self.dw5(x, training=training), training=training)\n\n        # ── concat + shuffle + fuse ─────────────────────────\n        out = tf.concat([b1, b2, b3], axis=-1)\n        out = channel_shuffle(out, self.groups)\n        out = self.bn_fuse(self.fuse(out), training=training)\n\n        # ── Channel Attention (min/max/avg/sum + learnable mod) ─\n        g_min = tf.reduce_min(out, axis=[1, 2])\n        g_max = tf.reduce_max(out, axis=[1, 2])\n        g_avg = tf.reduce_mean(out, axis=[1, 2])\n        g_sum = tf.reduce_sum(out, axis=[1, 2])\n\n        channel_add = g_min + g_avg + g_max + g_sum\n        channel_sub = g_max - g_avg - g_min\n        channel_info = channel_add + channel_sub\n\n        se = self.fc1(channel_info)\n        se = self.fc2(se)\n\n        channel_noise = tf.random.normal(shape=tf.shape(se), mean=0, \n                                         stddev=self.channel_noise_factor)\n\n        channel_noise *= self.channel_noise_weight + (channel_noise * self.channel_noise_weight)\n\n        se += channel_noise\n\n        se = tf.clip_by_value(se, 0, 1)\n\n\n        se = self.reshape(se)                    # (1,1,C)\n        se *= self.global_scale\n        se = tf.nn.sigmoid(se)\n        out1 = out * se\n\n        # ── residual projection & activation ────────────────\n        shortcut = self.bn_proj(self.proj(x), training=training)\n        out1 = self.act(shortcut + out1)\n\n\n\n\n\n        # Generate spatial and channel noise\n        spatial_noise = tf.random.normal(shape=tf.shape(out1), mean=0, \n                                         stddev=self.spatial_noise_factor)\n        \n        # Scale the noise with trainable weights\n        spatial_noise *= self.spatial_noise_weight + (spatial_noise * self.spatial_noise_weight)\n        \n\n        # Add spatial attention noise\n        out1 += spatial_noise\n\n\n        # Clip attention maps to ensure they are within the valid range [0, 1]\n        spatial_attention_output = tf.clip_by_value(out1, 0, 1)\n        \n\n        # Combine attention mechanisms\n        combined_attention1 = tf.multiply(spatial_attention_output, se)\n        combined_attention2 = tf.multiply(spatial_attention_output, se)\n        combined_attention3 = tf.multiply(spatial_attention_output, se)\n        combined_attention4 = tf.multiply(combined_attention1, combined_attention2)\n        combined_attention = tf.multiply(combined_attention3, combined_attention4)\n        \n        return tf.multiply(out, combined_attention)\n        \n        #return out\n\n\n# ────────────────────────────\n# ERLA block (RGSA) with RPF knobs surfaced\n# ────────────────────────────\ndef RGSA(x, filters, strides=(1, 1), use_projection=False,\n         rpf_ratio_gpc=0.0, rpf_ratio_dwc=0.0, rpf_l2=0.0):\n    shortcut = x\n\n    # First EMILA (MSGDC + CA), with optional downsampling\n    x = MultiKernelGroupWiseConv(filters=filters, groups=16, strides=strides,\n                                 rpf_ratio_gpc=rpf_ratio_gpc,\n                                 rpf_ratio_dwc=rpf_ratio_dwc,\n                                 rpf_l2=rpf_l2, spatial_noise_factor=0.6, channel_noise_factor=0.6)(x)\n\n    # BN + ReLU\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second EMILA (no downsampling)\n    x = MultiKernelGroupWiseConv(filters=filters, groups=16, strides=(1, 1),\n                                 rpf_ratio_gpc=rpf_ratio_gpc,\n                                 rpf_ratio_dwc=rpf_ratio_dwc,\n                                 rpf_l2=rpf_l2, spatial_noise_factor=0.6, channel_noise_factor=0.6)(x)\n\n    # 1×1 PW conv to mix channels\n    x = layers.Conv2D(filters, kernel_size=(1, 1), padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n\n    # Ensure residual shape match (if requested)\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding=\"same\")(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    x = layers.Add()([x, shortcut])\n    x = layers.Activation('relu')(x)\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\n\n## EMCAM + MAN\nclass DeeperGlobalLocalAttentionLayer1(layers.Layer):\n    def __init__(self, units, activation='sigmoid', dropout_rate=0.2, groups=16, spatial_noise_factor=0.6, channel_noise_factor=0.6, use_scale=True, **kwargs):\n        super(DeeperGlobalLocalAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_scale = use_scale\n        self.groups = groups\n        self.spatial_noise_factor = spatial_noise_factor\n        self.channel_noise_factor = channel_noise_factor\n\n\n    def build(self, input_shape):\n        input1, input2 = input_shape\n        _, _, _, channels1 = input1\n        _, _, _, channels2 = input2\n        \n        self.alpha1 = self.add_weight(shape=(1, 1, 1, channels1), initializer='ones', trainable=True)\n        self.alpha2 = self.add_weight(shape=(1, 1, 1, channels2), initializer='ones', trainable=True)\n        \n        self.global_min_pooling = GlobalMinPooling2D()\n        self.global_avg_pooling = layers.GlobalAveragePooling2D()\n        self.global_max_pooling = layers.GlobalMaxPooling2D()\n        \n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n        \n        self.local_conv1 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.local_conv2 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        \n        #self.min_pooling = MinPooling2D(pool_size = 1)\n        #self.min_pooling1 = MinPooling2D(pool_size = 1)\n        self.avg_pooling = layers.AveragePooling2D(pool_size = 1)\n        self.max_pooling = layers.MaxPooling2D(pool_size = 1)\n        \n        self.sub = layers.Subtract()\n        self.mod_scale1 = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True)\n        self.mod_scale2 = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True)\n        self.mod_scale3 = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True)\n\n        self.spatial_noise_weight = self.add_weight(shape=(1,), initializer='ones')\n        self.channel_noise_weight = self.add_weight(shape=(1,), initializer='ones')\n        \n        \n        self.msgdc_conv1x1 = layers.Conv2D(\n            self.units, 1, padding=\"same\", use_bias=False, groups=self.groups\n        )\n        self.msgdc_dw3 = layers.DepthwiseConv2D(\n            3, padding=\"same\", use_bias=False\n        )\n        self.msgdc_dw5 = layers.DepthwiseConv2D(\n            5, padding=\"same\", use_bias=False\n        )\n        self.msgdc_fuse = layers.Conv2D(\n            self.units, 1, padding=\"same\", use_bias=False\n        )\n        \n        if self.use_scale:\n            self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True, name='global_scale')\n            self.local_scale = self.add_weight(shape=(1, 1, 1, self.units), initializer='ones', trainable=True, name='local_scale')\n            \n            self.global_scale2 = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True, name='global_scale2')\n            self.local_scale2 = self.add_weight(shape=(1, 1, 1, self.units), initializer='ones', trainable=True, name='local_scale2')\n        \n        super(DeeperGlobalLocalAttentionLayer1, self).build(input_shape)\n\n    def MFIFA(self, x):\n            input_shape1 = tf.shape(x)\n            flattened_inputs1 = tf.reshape(x, [-1, input_shape1[-1]])  # Flatten along the last axis\n            dct_transformed1 = tf.signal.dct(flattened_inputs1, type=2, norm='ortho')\n            dct_transformed1 = tf.reshape(dct_transformed1, input_shape1)  # Reshape back to original dimensions\n            \n            High1 = self.global_max_pooling(dct_transformed1)   ## High 1\n            High1 = self.global_attention(High1)\n            High1 = tf.expand_dims(tf.expand_dims(High1, 1), 1)\n    \n            Low1 = self.global_avg_pooling(dct_transformed1)  ## Low 1\n            Low1 = self.global_attention(Low1)\n            Low1 = tf.expand_dims(tf.expand_dims(Low1, 1), 1)\n    \n            \n            Low2 = self.global_min_pooling(dct_transformed1)   ## Low 2\n            Low2 = self.global_attention(Low2)\n            Low2 = tf.expand_dims(tf.expand_dims(Low2, 1), 1)\n            \n            High2 = High1 - Low1\n            \n            High3 = dct_transformed1 - Low2\n    \n            High = High1 + High2 + High3\n            Low = Low1 + Low2\n            Mean = High - Low\n\n            #High = tf.expand_dims(tf.expand_dims(High, 1), 1)\n            #Low  = tf.expand_dims(tf.expand_dims(Low, 1), 1)\n            #Mean = tf.expand_dims(tf.expand_dims(Mean, 1), 1)\n    \n            High *= self.mod_scale1 # Modulation\n            Low *= self.mod_scale2 # Modulation\n            Mean *= self.mod_scale3 # Modulation\n            \n            return High, Low, Mean\n\n    \n    def _multi_kernel_groupwise_conv(self, x):\n        \"\"\"MSGDC block, *re-using* layers defined in build().\"\"\"\n        c1 = self.msgdc_conv1x1(x)\n        c3 = self.msgdc_dw3(x)\n        c5 = self.msgdc_dw5(x)\n\n        concat = tf.concat([c1, c3, c5], axis=-1)\n        return self.msgdc_fuse(concat)\n\n        \n\n    def call(self, inputs, training=None):\n        input1, input2 = inputs\n        \n        # MFIFA\n\n        High_1, Low_1, Mean_1 = self.MFIFA(input1) # for input modality 1\n        High_2, Low_2, Mean_2 = self.MFIFA(input2) # for input modality 2\n\n        \n        High = High_1 + High_2\n        Low = Low_1 + Low_2\n        Mean = Mean_1 + Mean_2\n        \n        attention = tf.sigmoid(High + Low + Mean)  # MFIFA Attention map generation\n        \n        \n        # Deeper Spatial Attention\n        \n        local_attention1 = self._multi_kernel_groupwise_conv(input1) #self.local_conv1(input1)\n        local_attention1_mean = tf.reduce_mean(local_attention1, axis=[1, 2], keepdims=True)\n        local_attention1_max = tf.reduce_max(local_attention1, axis=[1, 2], keepdims=True)\n        local_attention1 = local_attention1_max + local_attention1_mean\n\n        local_attention2 = self._multi_kernel_groupwise_conv(input2) #self.local_conv1(input1)\n        local_attention2_mean = tf.reduce_mean(local_attention2, axis=[1, 2], keepdims=True)\n        local_attention2_max = tf.reduce_max(local_attention2, axis=[1, 2], keepdims=True)\n        local_attention2 = local_attention2_max + local_attention2_mean\n        \n        \n        \n        local_attention1_1 = self._multi_kernel_groupwise_conv(local_attention1)\n        local_attention1_1_mean = tf.reduce_mean(local_attention1_1, axis=[1, 2], keepdims=True)\n        local_attention1_1_max = tf.reduce_max(local_attention1_1, axis=[1, 2], keepdims=True)\n        local_attention1 = local_attention1 + local_attention1_1_mean + local_attention1_1_max  + local_attention2 # Skip with Cross-modal Interactions\n\n        \n        \n        local_attention2_1 = self._multi_kernel_groupwise_conv(local_attention2)\n        local_attention2_1_mean = tf.reduce_mean(local_attention2_1, axis=[1, 2], keepdims=True)\n        local_attention2_1_max = tf.reduce_max(local_attention2_1, axis=[1, 2], keepdims=True)\n        local_attention2 = local_attention2 + local_attention2_1_mean + local_attention2_1_max  + local_attention1 # Skip with Cross-modal Interactions\n\n        \n        #local_attention2 = local_attention2_ + local_attention2__\n\n        # Scale Global and Spatial Attention\n    \n        \n        local_attention1 *= self.local_scale # Modulation\n        local_attention2 *= self.local_scale2 # Modulation\n\n        # Combine \n        attention1 = tf.sigmoid(local_attention1 + local_attention2) # EMSCA Attention map generation\n\n        attention *= self.global_scale # Modulation\n        attention1 *= self.local_scale # Modulation\n        \n        attention2 = tf.sigmoid(attention + attention1) # dUal domain Parallel Fusion Attention Map Generation via EMCAM\n\n\n\n\n\n        attention2_1 = attention2\n\n        spatial_noise = tf.random.normal(shape=tf.shape(attention2), mean=0, \n                                         stddev=self.spatial_noise_factor)\n        \n        # Scale the noise with trainable weights\n        spatial_noise *= self.spatial_noise_weight + (spatial_noise * self.spatial_noise_weight)\n        \n\n        # Add spatial attention noise\n        attention2 += spatial_noise\n\n\n        # Clip attention maps to ensure they are within the valid range [0, 1]\n        attention2 = tf.clip_by_value(attention2, 0, 1)\n        \n\n        # Combine attention mechanisms\n        combined_attention1 = tf.multiply(attention2, attention2_1)\n        combined_attention2 = tf.multiply(attention2, attention2)\n        combined_attention3 = tf.multiply(attention2, attention2)\n        combined_attention4 = tf.multiply(combined_attention1, combined_attention2)\n        combined_attention = tf.multiply(combined_attention3, combined_attention4)\n        \n        \n\n\n        \n        \n        \n        return combined_attention \n\n    def get_config(self):\n        config = super(DeeperGlobalLocalAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate,\n                       'use_scale': self.use_scale})\n        return config\n\n\n## EMCAM Attention Map Integration Into Input modalities\nclass DeeperAttentionLayer1(layers.Layer):\n    def __init__(self, units=64, use_scale=True, **kwargs):\n        super(DeeperAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.use_scale = use_scale\n\n    def build(self, input_shape):\n        input1, input2 = input_shape\n        _, H, W, C1 = input1\n        _, H, W, C2 = input2\n        \n        self.alpha3 = self.add_weight(shape=(1, 1, 1, C1), initializer='ones', trainable=True, name='alpha3')\n        self.alpha4 = self.add_weight(shape=(1, 1, 1, C2), initializer='ones', trainable=True, name='alpha4')\n        self.add = layers.Add()\n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer1(units=self.units, activation='sigmoid', \n                                                                              dropout_rate=0.2,\n                                                                              use_scale=self.use_scale)\n        super(DeeperAttentionLayer1, self).build(input_shape)\n\n        \n    def call(self, inputs, training=None):\n        input1, input2 = inputs\n        attention2 = self.deeper_global_local_attention(inputs, training=training)\n        attention_feature1 = input1 * attention2 * self.alpha3 # Attention Map Integration Into Input modality 1\n        attention_feature2 = input2 * attention2 * self.alpha4 # Attention Map Integration Into Input modality 2\n        \n        #attention_feature = self.add([attention_feature1, attention_feature2])\n        return attention_feature1, attention_feature2\n\n    def get_config(self):\n        config = super(DeeperAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'use_scale': self.use_scale})\n        return config\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## MSTL\ndef residual_GLC_branch1(inputs1, inputs2):\n    \n    x1 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs1)\n    #x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(64)(x1)\n    x1 = BatchNormalization()(x1)\n    x1 = tf.keras.layers.Activation('relu')(x1)\n    x1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x1)\n    \n    x2 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs2)\n    #x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2) ## MFA ####\n    #x2 = AttentionBlock(64)(x2)\n    x2 = BatchNormalization()(x2)\n    x2 = tf.keras.layers.Activation('relu')(x2)\n    x2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x2)\n    \n    ## ERLA\n    x1 = RGSA(x1, filters=64)\n    #\n    x2 = RGSA(x2, filters=64)\n    \n    \n    \n    \n    x1 = RGSA(x1, filters=64)\n    \n    x2 = RGSA(x2, filters=64)\n    \n    x1, x2 = DeeperAttentionLayer1(units=64, use_scale=True)([x1, x2])\n    \n    ## ERLA\n    x1 = RGSA(x1, filters=128, strides=(2, 2), use_projection=True)\n    \n    x2 = RGSA(x2, filters=128, strides=(2, 2), use_projection=True)\n    \n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True) \n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True) \n    \n    \n    \n    x1 = RGSA(x1, filters=128)\n    \n    x2 = RGSA(x2, filters=128)\n\n    \n    x1, x2 = DeeperAttentionLayer1(units=128, use_scale=True)([x1, x2])\n    \n    ## ERLA\n    x1 = RGSA(x1, filters=256, strides=(2, 2), use_projection=True)\n    \n    \n    x2 = RGSA(x2, filters=256, strides=(2, 2), use_projection=True)\n    \n\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True) \n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True) \n    \n    \n    \n    x1 = RGSA(x1, filters=256)\n    \n    x2 = RGSA(x2, filters=256)\n    \n    x1, x2 = DeeperAttentionLayer1(units=256, use_scale=True)([x1, x2])\n    \n    \n    ## ERLA\n    x1 = RGSA(x1, filters=512, strides=(2, 2), use_projection=True)\n    \n    x2 = RGSA(x2, filters=512, strides=(2, 2), use_projection=True)\n\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True) \n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True) \n    \n    \n    ## ERLA\n    x1 = RGSA(x1, filters=512)\n    x2 = RGSA(x2, filters=512)\n\n    x1, x2 = DeeperAttentionLayer1(units=512, use_scale=True)([x1, x2])\n    \n    \n    return x1, x2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def build_resnet18(input_shape=(128, 128, 3), num_classes=2):\ninput_shape=(128, 128, 3)\ninputs1 = Input(shape=input_shape)\ninputs2 = Input(shape=input_shape)\n\nimport tensorflow.keras.layers as L\n\n#input_data = Input(shape=input_shape, name='input_data')\n# Initial convolutional layer\n\nx1, x2 = residual_GLC_branch1(inputs1, inputs2)\n#print('x:',x.shape)\n\n\ncon = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n\n#con = tf.keras.layers.Dropout(0.25)(con) \n\nx = GlobalAveragePooling2D()(con)\n#print('GlobalAveragePooling2D x:',x.shape)\n\n## TMTL\noutputs1 = Dense(5, activation='softmax')(x)\noutputs2 = Dense(7, activation='softmax')(x)\n\n# Create the model\nmodel = Model([inputs1, inputs2], [outputs1, outputs2])\n#return model\nprint(model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.0001)\n#opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=80, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=70, verbose = 1, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\ndef pgd_attack_multi_input(\n    model,\n    X_list,\n    y_list,\n    epsilon,\n    alpha,\n    num_iter,\n    batch_size=32,\n    clip_min=0.0,\n    clip_max=1.0,\n    from_logits=False,\n):\n    \"\"\"\n    Projected Gradient Descent (PGD) for multi-input, multi-output tf.keras models.\n\n    Args:\n        model: tf.keras.Model with one or more inputs and one or more outputs.\n        X_list: list of Tensors/ndarrays, one per model input, shape (N, ...).\n        y_list: list of Tensors/ndarrays, one per model output, shape (N, ...).\n        epsilon: float or list[float], L_inf radius per input.\n        alpha: float or list[float], step size per input.\n        num_iter: number of PGD steps.\n        batch_size: minibatch size.\n        clip_min, clip_max: value range for inputs (usually 0..1).\n        from_logits: set True if model outputs logits (not probabilities).\n\n    Returns:\n        adv_X_list: list of Tensors, one per input, adversarial examples.\n    \"\"\"\n    # --- Standardize containers & dtypes ---\n    X_list = [tf.convert_to_tensor(x, dtype=tf.float32) for x in X_list]\n    y_list = [tf.convert_to_tensor(y, dtype=tf.float32) for y in y_list]\n\n    num_inputs = len(X_list)\n    num_outputs = len(y_list)\n\n    # Broadcast epsilon/alpha to per-input lists if provided as scalars\n    if isinstance(epsilon, (list, tuple)):\n        eps_list = [tf.cast(e, tf.float32) for e in epsilon]\n        assert len(eps_list) == num_inputs, \"epsilon list must match number of inputs\"\n    else:\n        eps_list = [tf.cast(epsilon, tf.float32)] * num_inputs\n\n    if isinstance(alpha, (list, tuple)):\n        alpha_list = [tf.cast(a, tf.float32) for a in alpha]\n        assert len(alpha_list) == num_inputs, \"alpha list must match number of inputs\"\n    else:\n        alpha_list = [tf.cast(alpha, tf.float32)] * num_inputs\n\n    # Dataset size (assumes fully known static first dimension)\n    num_samples = int(X_list[0].shape[0])\n    for X in X_list[1:]:\n        assert int(X.shape[0]) == num_samples, \"All inputs must have the same number of samples\"\n\n    # Collect adversarial batches to concat at the end\n    adv_batches_per_input = [[] for _ in range(num_inputs)]\n\n    # --- Iterate over minibatches ---\n    for start in range(0, num_samples, batch_size):\n        end = min(start + batch_size, num_samples)\n\n        # Clean batch (used as center for projection)\n        x0_batch_list = [tf.identity(X[start:end]) for X in X_list]\n        yb_list = [y[start:end] for y in y_list]\n\n        # Variables we will update in-place\n        adv_batch_vars = [tf.Variable(xb) for xb in x0_batch_list]\n\n        for _ in range(num_iter):\n            with tf.GradientTape() as tape:\n                # Forward pass\n                preds = model([v for v in adv_batch_vars], training=False)\n                preds_list = list(preds) if isinstance(preds, (list, tuple)) else [preds]\n                if len(preds_list) != num_outputs:\n                    raise ValueError(\n                        f\"Model produced {len(preds_list)} outputs but y_list has {num_outputs}.\"\n                    )\n\n                # Per-output losses\n                losses = []\n                for y_true, y_pred in zip(yb_list, preds_list):\n                    # Choose binary vs categorical based on last dimension of y_true\n                    if (y_true.shape.rank is not None and\n                        y_true.shape[-1] is not None and\n                        int(y_true.shape[-1]) > 1):\n                        # one-hot multi-class (including one-hot binary)\n                        l = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n                    else:\n                        # binary with shape (..., 1) or scalar labels\n                        l = tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=from_logits)\n                    losses.append(tf.reduce_mean(l))\n\n                total_loss = tf.add_n(losses)\n\n            # Backward pass\n            grads = tape.gradient(total_loss, adv_batch_vars)\n\n            # Defensive: replace None grads (disconnected inputs) with zeros\n            grads = [tf.zeros_like(v) if g is None else g for g, v in zip(grads, adv_batch_vars)]\n\n            # PGD step + projection\n            for i in range(num_inputs):\n                # Gradient sign step\n                adv_batch_vars[i].assign_add(alpha_list[i] * tf.sign(grads[i]))\n                # Project onto L_inf ball around the ORIGINAL clean batch\n                adv_batch_vars[i].assign(\n                    tf.clip_by_value(\n                        adv_batch_vars[i],\n                        x0_batch_list[i] - eps_list[i],\n                        x0_batch_list[i] + eps_list[i],\n                    )\n                )\n                # Clip to valid data range\n                adv_batch_vars[i].assign(\n                    tf.clip_by_value(adv_batch_vars[i], clip_min, clip_max)\n                )\n\n        # Stash the adversarial minibatch\n        for i in range(num_inputs):\n            adv_batches_per_input[i].append(adv_batch_vars[i].read_value())\n\n    # Concatenate minibatches to full tensors\n    adv_X_list = [tf.concat(adv_batches_per_input[i], axis=0) for i in range(num_inputs)]\n    return adv_X_list\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score\n\n# Your model\nmodel = model1\n\n# Hyperparameters\nepsilon_values = [4/255]  # ~0.015686\nbatch_size = 10\nnum_iter = 10\n\nfor epsilon in epsilon_values:\n    alpha = epsilon / 4.0\n\n    adv_X_list = pgd_attack_multi_input(\n        model=model,\n        X_list=[X_test_s, X_test_h1],\n        y_list=[y_test_s, y_test_h1],\n        epsilon=epsilon,\n        alpha=alpha,\n        num_iter=num_iter,\n        batch_size=batch_size,\n        clip_min=0.0,\n        clip_max=1.0,\n        from_logits=False,  # <-- set True if your model outputs logits\n    )\n\n    # Predict on adversarial examples\n    y_pred_list = model.predict(adv_X_list, verbose=0)\n    y_pred_list = list(y_pred_list) if isinstance(y_pred_list, (list, tuple)) else [y_pred_list]\n\n    # --- Helper to get class labels robustly (binary or multi-class) ---\n    def to_class_labels(y_true, y_pred):\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n\n        # One-hot case (binary or multi-class)\n        if y_true.ndim > 1 and y_true.shape[-1] > 1:\n            y_true_cls = np.argmax(y_true, axis=1)\n            y_pred_cls = np.argmax(y_pred, axis=1)\n            return y_true_cls, y_pred_cls\n\n        # Binary case with shape (N,) or (N,1)\n        y_true_bin = (y_true.reshape(-1) >= 0.5).astype(int)\n        if y_pred.ndim > 1 and y_pred.shape[-1] > 1:\n            # model used 2-class softmax\n            y_pred_cls = np.argmax(y_pred, axis=1)\n        else:\n            # model used single sigmoid head\n            y_pred_cls = (y_pred.reshape(-1) >= 0.5).astype(int)\n        return y_true_bin, y_pred_cls\n\n    # Output 0 metrics\n    y_true_0, y_pred_0 = to_class_labels(y_test_s, y_pred_list[0])\n    acc_0 = accuracy_score(y_true_0, y_pred_0)\n\n    # Output 1 metrics\n    y_true_1, y_pred_1 = to_class_labels(y_test_h1, y_pred_list[1])\n    acc_1 = accuracy_score(y_true_1, y_pred_1)\n\n    print(f\"Epsilon: {epsilon:.6f}\")\n    print(f\"Accuracy (output 0): {acc_0:.4f}\")\n    print(f\"Accuracy (output 1): {acc_1:.4f}\")\n    print(\"-\" * 50)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}